"""
æ¢—æ–‡åŒ–æ•°æ®æ¸…æ´—æ¨¡å—
å®ç°å®Œæ•´çš„æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†æµç¨‹
"""
import re
import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import hashlib
import jieba
import jieba.analyse
from urllib.parse import urlparse
import json

from database.models import RawPost

logger = logging.getLogger(__name__)

class MemeDataCleaner:
    """æ¢—æ–‡åŒ–æ•°æ®æ¸…æ´—å™¨"""
    
    def __init__(self):
        # åˆå§‹åŒ–jiebaåˆ†è¯
        self._init_jieba()
        
        # æ¢—ç›¸å…³çš„åœç”¨è¯
        self.stopwords = {
            "çš„", "æ˜¯", "äº†", "åœ¨", "æœ‰", "å’Œ", "å°±", "éƒ½", "è€Œ", "åŠ", "ä¸", "æˆ–",
            "ä¸€ä¸ª", "è¿™ä¸ª", "é‚£ä¸ª", "ä»€ä¹ˆ", "æ€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "å¦‚ä½•", "å¤šå°‘",
            "å¾ˆ", "éå¸¸", "å¤ª", "çœŸ", "ç¡®å®", "çœŸçš„", "æ„Ÿè§‰", "è§‰å¾—", "çœ‹èµ·æ¥",
            "è¯´", "çœ‹", "å¬", "æƒ³", "çŸ¥é“", "äº†è§£", "æ˜ç™½", "ç†è§£",
            "å§", "å‘¢", "å•Š", "å“¦", "é¢", "å‘ƒ", "å—¯", "é¢", "è¯¶"
        }
        
        # æ¢—è¯†åˆ«å…³é”®è¯
        self.meme_keywords = {
            "æµè¡Œè¯­": ["æ¢—", "meme", "ç½‘ç»œç”¨è¯­", "æµè¡Œè¯­", "å£å¤´ç¦…", "ç½‘ç»œæ¢—"],
            "è¡¨æƒ…åŒ…": ["è¡¨æƒ…åŒ…", "è¡¨æƒ…", "emoji", "æ»‘ç¨½", "ç‹—å¤´", "ä¿å‘½"],
            "è§†é¢‘æ¢—": ["è§†é¢‘", "ç‰‡æ®µ", "å‰ªè¾‘", "é¬¼ç•œ", "é¬¼ç•œè§†é¢‘", "é­”æ€§"],
            "æ–‡å­—æ¢—": ["æ®µå­", "ç¬‘è¯", "æç¬‘", "å¹½é»˜", "æ²™é›•", "æœ‰è¶£"],
            "äºŒæ¬¡å…ƒ": ["äºŒæ¬¡å…ƒ", "åŠ¨æ¼«", "ç•ªå‰§", "èŒ", "å¯çˆ±", "è€å©†", "è€å…¬"],
            "æ¸¸æˆ": ["æ¸¸æˆ", "ç”µç«", "é˜Ÿå‹", "çŒªé˜Ÿå‹", "ç¥æ“ä½œ", "èœ"],
            "ç½‘ç»œæ–‡åŒ–": ["ç½‘ç»œæ–‡åŒ–", "ç½‘ç»œæµè¡Œ", "å½“ä»£é’å¹´", "ç²¾ç¥å°ä¼™", "ç¤¾ä¼š"]
        }
        
        # æƒ…æ„Ÿè¯æ±‡
        self.sentiment_words = {
            "positive": ["èµ", "å¥½", "æ£’", "ä¼˜ç§€", "å‰å®³", "666", "ç‰›", "çˆ±äº†", "å¤ªæ£’äº†"],
            "negative": ["åƒåœ¾", "å·®", "çƒ‚", "ä¸è¡Œ", "è®¨åŒ", "æ¶å¿ƒ", "æƒ³å", "å—ä¸äº†"],
            "neutral": ["ä¸€èˆ¬", "æ™®é€š", "è¿˜è¡Œ", "å‡‘åˆ", "é©¬é©¬è™è™"]
        }
    
    def _init_jieba(self):
        """åˆå§‹åŒ–jiebaåˆ†è¯è¯å…¸"""
        # æ·»åŠ ç½‘ç»œç”¨è¯­åˆ°è¯å…¸
        jieba.add_word("æ¢—", tag='n')
        jieba.add_word("meme", tag='n')
        jieba.add_word("è¡¨æƒ…åŒ…", tag='n')
        jieba.add_word("æ²™é›•", tag='n')
        jieba.add_word("é­”æ€§", tag='adj')
        jieba.add_word("é¬¼ç•œ", tag='n')
        jieba.add_word("äºŒæ¬¡å…ƒ", tag='n')
        jieba.add_word("ç²¾ç¥å°ä¼™", tag='n')
        jieba.add_word("ç¤¾ä¼šè¯­å½•", tag='n')
        
        # åŠ è½½åœç”¨è¯æ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
        try:
            with open("data/stopwords.txt", "r", encoding="utf-8") as f:
                custom_stopwords = [line.strip() for line in f if line.strip()]
                self.stopwords.update(custom_stopwords)
        except FileNotFoundError:
            logger.info("Using default stopwords")
    
    def clean_raw_post(self, raw_post: RawPost) -> Dict[str, Any]:
        """æ¸…æ´—å•ä¸ªåŸå§‹å¸–å­"""
        try:
            cleaned_data = {
                "id": raw_post.id,
                "platform": raw_post.platform,
                "url": raw_post.url,
                "content": self._clean_content(raw_post.content),
                "title": self._clean_title(raw_post.title) if raw_post.title else "",
                "author": self._clean_author(raw_post.author) if raw_post.author else "",
                "timestamp": raw_post.timestamp,
                "engagement": self._calculate_engagement(raw_post),
                "sentiment": self._analyze_sentiment(raw_post.content),
                "keywords": self._extract_keywords(raw_post.content),
                "meme_type": self._identify_meme_type(raw_post.content),
                "quality_score": self._calculate_quality_score(raw_post),
                "processed_at": datetime.now()
            }
            
            return cleaned_data
            
        except Exception as e:
            logger.error(f"Error cleaning raw post {raw_post.id}: {e}")
            return None
    
    def clean_batch_posts(self, raw_posts: List[RawPost]) -> List[Dict[str, Any]]:
        """æ‰¹é‡æ¸…æ´—å¸–å­æ•°æ®"""
        cleaned_posts = []
        
        for post in raw_posts:
            cleaned = self.clean_raw_post(post)
            if cleaned:
                cleaned_posts.append(cleaned)
        
        logger.info(f"Cleaned {len(cleaned_posts)} out of {len(raw_posts)} posts")
        return cleaned_posts
    
    def _clean_content(self, content: str) -> str:
        """æ¸…æ´—å†…å®¹æ–‡æœ¬"""
        if not content:
            return ""
        
        # ç§»é™¤URL
        content = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', content)
        
        # ç§»é™¤@ç”¨æˆ·å
        content = re.sub(r'@[\\w]+', '', content)
        
        # ç§»é™¤è¯é¢˜æ ‡ç­¾
        content = re.sub(r'#([^#]+)#', r'\\1', content)
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        content = re.sub(r'\\s+', ' ', content)
        
        # ç§»é™¤è¡¨æƒ…ç¬¦å·çš„ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™åŸºæœ¬æ ‡ç‚¹ï¼‰
        content = re.sub(r'[ğŸ‰ğŸŠğŸˆğŸğŸ‚ğŸ„ğŸ…ğŸ†ğŸ‡ğŸŒŸâ­ğŸ’«âœ¨ğŸš€ğŸ¯ğŸªğŸ­ğŸ¨ğŸ¬ğŸµğŸ¶ğŸ¼ğŸ¹ğŸ¸ğŸ¥ğŸ¤ğŸ§]', '', content)
        
        # ç§»é™¤é‡å¤çš„æ ‡ç‚¹ç¬¦å·
        content = re.sub(r'([!?.ã€‚]{2,})', lambda m: m.group(1)[0], content)
        
        return content.strip()
    
    def _clean_title(self, title: str) -> str:
        """æ¸…æ´—æ ‡é¢˜"""
        if not title:
            return ""
        
        # ç§»é™¤è¿‡é•¿çš„æ ‡é¢˜
        if len(title) > 100:
            title = title[:97] + "..."
        
        # ç§»é™¤ç‰¹æ®Šç¬¦å·
        title = re.sub(r'[ğŸ“ŒğŸ“ğŸ”¥ğŸ’¯ğŸ‘‘ğŸ¯ğŸŠ]', '', title)
        
        return title.strip()
    
    def _clean_author(self, author: str) -> str:
        """æ¸…æ´—ä½œè€…ä¿¡æ¯"""
        if not author:
            return ""
        
        # ç§»é™¤ç‰¹æ®Šå‰ç¼€
        author = re.sub(r'^(ç”¨æˆ·|ç”¨æˆ·|ç½‘å‹|åšä¸»|UPä¸»|ä½œè€…|è´¦å·)[::ï¼š]?', '', author)
        
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        author = re.sub(r'\\s+', ' ', author)
        
        return author.strip()
    
    def _calculate_engagement(self, raw_post: RawPost) -> Dict[str, Any]:
        """è®¡ç®—å‚ä¸åº¦æŒ‡æ ‡"""
        # è®¡ç®—å‚ä¸ç‡
        total_interactions = (
            raw_post.like_count + 
            raw_post.comment_count + 
            raw_post.share_count +
            raw_post.upvotes - raw_post.downvotes
        )
        
        # å‚ä¸åº¦åˆ†æ•°ï¼ˆ0-1ä¹‹é—´ï¼‰
        engagement_score = min(1.0, total_interactions / 1000.0)
        
        return {
            "like_count": raw_post.like_count,
            "comment_count": raw_post.comment_count,
            "share_count": raw_post.share_count,
            "upvotes": raw_post.upvotes,
            "downvotes": raw_post.downvotes,
            "total_interactions": total_interactions,
            "engagement_score": engagement_score
        }
    
    def _analyze_sentiment(self, content: str) -> Dict[str, Any]:
        """åˆ†ææƒ…æ„Ÿå€¾å‘"""
        if not content:
            return {"sentiment": "neutral", "score": 0.0}
        
        positive_count = 0
        negative_count = 0
        
        # è®¡ç®—æ­£é¢å’Œè´Ÿé¢è¯æ±‡æ•°é‡
        for word in self.sentiment_words["positive"]:
            positive_count += content.count(word)
        
        for word in self.sentiment_words["negative"]:
            negative_count += content.count(word)
        
        # è®¡ç®—æƒ…æ„Ÿåˆ†æ•°
        total_sentiment_words = positive_count + negative_count
        if total_sentiment_words == 0:
            sentiment_score = 0.0
            sentiment = "neutral"
        else:
            sentiment_score = (positive_count - negative_count) / total_sentiment_words
            if sentiment_score > 0.1:
                sentiment = "positive"
            elif sentiment_score < -0.1:
                sentiment = "negative"
            else:
                sentiment = "neutral"
        
        return {
            "sentiment": sentiment,
            "score": sentiment_score,
            "positive_indicators": positive_count,
            "negative_indicators": negative_count
        }
    
    def _extract_keywords(self, content: str, top_k: int = 10) -> List[str]:
        """æå–å…³é”®è¯"""
        if not content:
            return []
        
        # ä½¿ç”¨jiebaåˆ†è¯
        words = jieba.cut(content)
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        filtered_words = [
            word for word in words 
            if len(word) >= 2 and word not in self.stopwords
        ]
        
        # è®¡ç®—è¯é¢‘
        word_freq = {}
        for word in filtered_words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        # æŒ‰é¢‘ç‡æ’åºå¹¶è¿”å›å‰kä¸ª
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        return [word for word, freq in sorted_words[:top_k]]
    
    def _identify_meme_type(self, content: str) -> Optional[str]:
        """è¯†åˆ«æ¢—ç±»å‹"""
        if not content:
            return None
        
        content_lower = content.lower()
        
        # è®¡ç®—æ¯ç§ç±»å‹çš„åŒ¹é…åº¦
        type_scores = {}
        for meme_type, keywords in self.meme_keywords.items():
            score = 0
            for keyword in keywords:
                score += content_lower.count(keyword.lower())
            type_scores[meme_type] = score
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„ç±»å‹
        if type_scores and max(type_scores.values()) > 0:
            return max(type_scores.items(), key=lambda x: x[1])[0]
        
        return "general"
    
    def _calculate_quality_score(self, raw_post: RawPost) -> float:
        """è®¡ç®—å†…å®¹è´¨é‡åˆ†æ•°"""
        score = 0.0
        
        # å†…å®¹é•¿åº¦åˆ†æ•°ï¼ˆ10-500å­—ç¬¦ä¸ºæ»¡åˆ†ï¼‰
        content_length = len(raw_post.content)
        if 10 <= content_length <= 500:
            score += 0.3
        elif content_length > 0:
            score += 0.1
        
        # å‚ä¸åº¦åˆ†æ•°
        total_engagement = (
            raw_post.like_count + 
            raw_post.comment_count + 
            raw_post.share_count
        )
        if total_engagement > 100:
            score += 0.3
        elif total_engagement > 10:
            score += 0.2
        elif total_engagement > 0:
            score += 0.1
        
        # æ—¶é—´æ–°é²œåº¦åˆ†æ•°ï¼ˆ24å°æ—¶å†…çš„å†…å®¹åŠ åˆ†ï¼‰
        if raw_post.timestamp:
            hours_old = (datetime.now() - raw_post.timestamp).total_seconds() / 3600
            if hours_old <= 24:
                score += 0.2
            elif hours_old <= 168:  # ä¸€å‘¨å†…
                score += 0.1
        
        # å¹³å°ç‰¹å®šåŠ åˆ†
        platform_scores = {
            "bilibili": 0.1,
            "weibo": 0.1,
            "zhihu": 0.1,
            "tieba": 0.1,
            "douyin": 0.1
        }
        score += platform_scores.get(raw_post.platform, 0)
        
        # å†…å®¹è´¨é‡æŒ‡æ ‡
        if raw_post.title and len(raw_post.title) > 5:
            score += 0.1
        
        return min(1.0, score)
    
    def deduplicate_posts(self, posts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é™¤é‡å¤å†…å®¹"""
        seen_content_hashes = set()
        deduplicated_posts = []
        
        for post in posts:
            # åˆ›å»ºå†…å®¹å“ˆå¸Œ
            content = f"{post.get('content', '')}{post.get('title', '')}"
            content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()
            
            if content_hash not in seen_content_hashes:
                seen_content_hashes.add(content_hash)
                deduplicated_posts.append(post)
        
        logger.info(f"Deduplicated {len(posts)} posts to {len(deduplicated_posts)} unique posts")
        return deduplicated_posts
    
    def filter_by_quality(self, posts: List[Dict[str, Any]], min_quality: float = 0.3) -> List[Dict[str, Any]]:
        """æŒ‰è´¨é‡è¿‡æ»¤å†…å®¹"""
        filtered_posts = [
            post for post in posts 
            if post.get('quality_score', 0.0) >= min_quality
        ]
        
        logger.info(f"Filtered {len(posts)} posts by quality to {len(filtered_posts)} posts")
        return filtered_posts
    
    def cluster_similar_memes(self, posts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """èšç±»ç›¸ä¼¼çš„æ¢—å†…å®¹"""
        if len(posts) < 2:
            return posts
        
        # ç®€å•çš„åŸºäºå…³é”®è¯ç›¸ä¼¼åº¦çš„èšç±»
        clusters = {}
        processed_ids = set()
        
        for post in posts:
            if post['id'] in processed_ids:
                continue
            
            # åˆ›å»ºæ–°èšç±»
            cluster_id = len(clusters)
            clusters[cluster_id] = {
                "cluster_id": cluster_id,
                "posts": [post],
                "representative": post,
                "keywords": post.get('keywords', [])[:5]
            }
            
            processed_ids.add(post['id'])
            
            # å¯»æ‰¾ç›¸ä¼¼å†…å®¹
            post_keywords = set(post.get('keywords', []))
            
            for other_post in posts:
                if other_post['id'] in processed_ids or other_post['id'] == post['id']:
                    continue
                
                other_keywords = set(other_post.get('keywords', []))
                
                # è®¡ç®—å…³é”®è¯é‡å åº¦
                if post_keywords and other_keywords:
                    overlap = len(post_keywords & other_keywords)
                    similarity = overlap / max(len(post_keywords), len(other_keywords))
                    
                    if similarity > 0.5:  # 50%ç›¸ä¼¼åº¦é˜ˆå€¼
                        clusters[cluster_id]["posts"].append(other_post)
                        processed_ids.add(other_post['id'])
        
        # ç”Ÿæˆèšç±»ç»“æœ
        clustered_posts = []
        for cluster in clusters.values():
            if len(cluster["posts"]) > 1:
                # å¤šå†…å®¹èšç±»ï¼Œä¿ç•™æœ€å…·ä»£è¡¨æ€§çš„
                best_post = max(cluster["posts"], key=lambda x: x.get('quality_score', 0))
                best_post['cluster_info'] = {
                    "cluster_id": cluster["cluster_id"],
                    "similar_posts_count": len(cluster["posts"]) - 1,
                    "representative_keywords": cluster["keywords"]
                }
                clustered_posts.append(best_post)
            else:
                # å•å†…å®¹ï¼Œç›´æ¥æ·»åŠ 
                clustered_posts.append(cluster["posts"][0])
        
        logger.info(f"Clustered {len(posts)} posts into {len(clustered_posts)} representative posts")
        return clustered_posts

# å…¨å±€å®ä¾‹
data_cleaner = MemeDataCleaner()